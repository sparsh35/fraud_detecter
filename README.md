# Money Laundering Prediction System

This model aims to streamline the manual decision-making process of a risk analyst in identifying potential cases of money laundering. It leverages automation to analyze events generated by a software, which captures user behavior on a platform. The model considers various features, including the amount of dollars received (total_income_dollar_amount), the amount of dollars sent (total_outcome_dollar_amount), and a risk scale assigned to the user (risk_pld). Utilizing these numerical and categorical data, the model assigns a probability (ranging from 0 to 1) indicating the likelihood of the evaluated user being involved in money laundering.


## Features
- [x] All the project dependencies and its versions can be found on the *requirements.txt* file.
- [x] The model can be used as a service or a ***Live scoring Model*** which lives in an EC2 instance on AWS
- [x] The model is served through a *Flask* Web Application using *Nginx* as a Web Server and *Guinicorn* as a WSGI HTTTP Server
- [x] Every training of the model generates a new experiment and registers a new version of the model in the model Registry at *MLFlow*.


- [x] MLFlow is also running on a separate *EC2* instance, so you can see the list of experiments of the model training . By default, the last trained model will be promoted as the "Productive model", it means that the last trained model will be used by the EC2 instance with the web application to make predictions with the incoming requests data. Previous productive models will be assigned a stage None.
- [x] MLFlow is storing its data on a *RDS instance* and all the generated artifcats during training and experimentation are saved on a *S3 Bucket* called "mlflow-artifact-remote-isa".


- [x] The model is a *XGBClassifier* which uses preprocessed data in a pipeline union (numeric and categorical data). This preprocessing has been customed through the use of *Sklearn* custom transformers. You can find all the custom transformers on the transformers.py file.
- [x] Hyperparamers, model settings, target column, numerical and categorical feature column names and other constant values can be found on the settings.py file.
- [x] I have set values for secrets as AWS Credentials, Bucket Names, Hosts, Paths and Prefect Keys on an .env file that is not available in the repo for security reasons.
- [x] **Pre-commit checks include: ** Detect private keys and aws credentials and styling rules as trailing whitespace, black, isort and pylint.
- [x] Everytime you send a POST request to the *Live Scoring Model* or *Online Model* to get a prediction you will get the result or results of the probability of money laundering for each case and the body response will also include the model metadata so can be sure about the model that was used to perform the prediction, this values can be compared to the ones saved on MLFlow.
- [x] *Evidently reports* were used to check Data Quality during each training. The *DataQualityPreset* checks the amount of columns, rows, nan values, unique values among other metrics that are stored on a *PostgreSQL* DB living at an EC2 instance.
- [x] Docker compose file was used to create two containers inside the EC2 instance with the model to save and serve the data generated by the *Evidently Reports*. Each training sends this data to the EC2 instance to save it on the DB and serve it through.


- [x] *Makefile* runs black and isort checks before starting a new training of the model through the run.sh file.
- [x] run.sh file sets the TRACKING_SERVER_HOST environment variable and runs the train.py file to get a new trained version of the model.
- [x] Deployments and training pipeline are handled using *Prefect Cloud*. It means you can follow all the steps of the training on the flow runs screen and create a new deployment from the front of the Prefect web app. Prefect cloud was configured on the EC2 instance, however, due to timeout reasons I will not use it on the demo to trigger deployments, but locally.
- [x] Data used for the model training is being downloaded from S3 using the *boto3* module. This data cannot be published for reasons of privacy of the source.
- [x] The Live scoring model with Gunicorn and the docker containers running Grafana and Adminer are being executed as services inside the EC2 instance that can be started, restarted or stopped through the *systemctl* command.

## Tools used

Tool  | Use Case
------------- | -------------
Flask | Python web service to process the POST requests, extract the body data, call the current productive model on MLFlow, perform the predictions using it and return the response to the user.
NGINX | Web Server Container to process incoming connections on port 80.
Gunicorn | WSGI HTTP Server.
Docker | To containerize the PostgreSQL database service and grafana dashboards.
Adminer | To access PostgreSQL data saved from Evidently Reports and AUC ROC.
Evidently | To check DataQuality through the DataQualityPreset.
Grafana | Dashboards to show DataQuality reports data by model version and to alert about low ROC AUC values on test dataset.
PostgreSQL | To store Evidently report results.
RDS | To store MLFlow data.
S3 Buckets | To store original data for training and resulting artifacts generated by MLFlow on each experiment.
EC2  | One to serve MLFlow and other one to serve the Live Scoring Model and the Docker containers with Adminer and Grafana.
MLFlow | Model store and Model registry.
Prefect and Prefect Cloud | To handle training runs and deployments.



**1. Data Generation**:
- Download data from S3 Bucket.
- Split data into train and test datasets.
- Validate that resulting dataframes are not empty.

**2. Train Laundering Money Model**:
- Creation of Pipeline with custom transformers, feature union and GridSearch.
- Fit the model Pipeline using the data generated previously.
- Calculate ROC AUC metrics for both datasets (train and test).

**3. Generate Evidently Reports**:
- Prepare DB to save the data. It creates a connection with the EC2 instance to verify the existence of the table that will store the data.
- **3.1 Generate Report:**
 - Run the DataQualityPreset report on the train (reference values) and test (current values) data using a custom column mapping.
 - Save results of the report as well as the ROC AUC calculated previously, on the PostgreSQL DB living in the Docker container at the EC2 instance.

**4. Save last trained model**: Creates a new version of the model in the model registry at MLFlow and set its stage as **Production** to be used by the Flask Application.


 ## Links
 Credentials to access Adminer:

 DB Type  |  User  | Password | DB Name
------------- | -------------  | ------------- | -------------
PostgreSQL | postgres | example | test

 Credentials to access Grafana:

User  | Password
------------- | -------------
admin | 123456




 ## Environment Variables


| Key  | Value  |
| :------------ | :------------ |
| TRACKING_SERVER_HOST  | ec2-18-206-253-206.compute-1.amazonaws.com  |
| DB_HOST  | 54.173.97.152  |

## Instructions

To install all the required dependencies you should have a Python environment already created (3.11 version). Then run from the root folder of the project:

`$ pip install -r requirements.txt`

To run one experiment and create a new version of the model on MLFlow and automatically serve it on the EC2 instance run:

`$ make run_train` or just `$ python train.py`

**Note: **The demo video illustrates how it works, however, since the data used to train the model has privacy policies you will not be available to use it locally.

To run a worker to receive deployments from Prefect:

`$ prefect worker start --pool 'mlops-project-pool'`


## Try it on Postman!
You can test the online model on the EC2 instance by sending a POST request using Postman with a body that follows the structure shown below:

```json
{
    "total_outcome_dollar_amount": [2109.19],
    "total_income_dollar_amount": [1305.06],
    "risk_pld": ["HIGH"]
}
```

